{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdwD8Fr0Nmry"
      },
      "outputs": [],
      "source": [
        "#Importing libraries\n",
        "\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data and Data Preparation\n",
        "dataset = load_dataset(\"humarin/chatgpt-paraphrases\")\n",
        "dataset = dataset.remove_columns(['category', 'source'])\n",
        "dataset = dataset['train']\n",
        "\n",
        "\n",
        "\n",
        "new_dataset = []\n",
        "\n",
        "for example in dataset:\n",
        "    text = example['text']\n",
        "    paraphrases = ast.literal_eval(example['paraphrases'])  # Extract all paraphrases\n",
        "    if paraphrases:  # Check if there are any paraphrases\n",
        "        first_paraphrase = paraphrases[0]  # Retain only the first paraphrase\n",
        "        new_example = (text, first_paraphrase)\n",
        "        new_dataset.append(new_example)\n",
        "\n",
        "X = [example[0] for example in new_dataset]\n",
        "Y = [example[1] for example in new_dataset]"
      ],
      "metadata": {
        "id": "49FTxyplNoY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "X_seq = tokenizer.texts_to_sequences(X)\n",
        "Y_seq = tokenizer.texts_to_sequences(Y)\n",
        "\n",
        "\n",
        "max_length = max(len(seq) for seq in X_seq)\n",
        "X_padded = pad_sequences(X_seq, maxlen=max_length, padding='post')\n",
        "Y_padded = pad_sequences(Y_seq, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "TyXjzS6COt5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y_padded, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.long)\n",
        "Y_train = torch.tensor(Y_train, dtype=torch.long)\n",
        "Y_test = torch.tensor(Y_test, dtype=torch.long)\n",
        "\n",
        "\n",
        "train_dataset = TensorDataset(X_train, Y_train)\n",
        "test_dataset = TensorDataset(X_test, Y_test)\n",
        "\n",
        "\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "pQlVer_ePk6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining Model Architecture\n",
        "\n",
        "class ParaphraserModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads):\n",
        "        super(ParaphraserModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.pre_attn_bilstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True)\n",
        "        self.post_attn_lstm = nn.LSTM(hidden_dim * 2, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.attention = nn.MultiheadAttention(hidden_dim * 2, num_heads)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "        self.dense = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        pre_attn_output, _ = self.pre_attn_bilstm(embedded)\n",
        "        attn_output, _ = self.attention(pre_attn_output, pre_attn_output, pre_attn_output)\n",
        "        post_attn_output, _ = self.post_attn_lstm(attn_output)\n",
        "        softmax_output = self.softmax(post_attn_output[:, -1, :])  # Use the last hidden state\n",
        "        dense_output = self.dense(softmax_output)\n",
        "        return dense_output\n",
        "        return dense_output\n",
        "\n"
      ],
      "metadata": {
        "id": "1IJrfMR7i7K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Training the model\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 256\n",
        "num_layers = 2\n",
        "num_heads = 8\n",
        "\n",
        "model = ParaphraserModel(vocab_size, embedding_dim, hidden_dim, num_layers, num_heads).to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for batch_inputs, batch_targets in train_loader:\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_targets[:, -1])  # Use the last token as the target\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
      ],
      "metadata": {
        "id": "mVtp8ryajApK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for batch_inputs, batch_targets in test_loader:\n",
        "        batch_inputs = batch_inputs.to(device)\n",
        "        batch_targets = batch_targets.to(device)\n",
        "\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_targets[:, -1])\n",
        "        total_loss += loss.item() * batch_inputs.size(0)\n",
        "\n",
        "        _, predicted = torch.max(outputs, dim=-1)\n",
        "        total_correct += (predicted == batch_targets[:, -1]).sum().item()\n",
        "        total_samples += batch_inputs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(test_dataset)\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f\"Test Loss: {avg_loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "2W7F3RUPjMbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation metrics\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "def calculate_metrics(model, tokenizer, test_loader, device):\n",
        "    model.eval()\n",
        "    bleu_scores = []\n",
        "    semantic_similarities = []\n",
        "    perplexities = []\n",
        "\n",
        "    sentence_transformer = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "    gpt2_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_inputs, batch_targets in test_loader:\n",
        "            batch_inputs = batch_inputs.to(device)\n",
        "            batch_targets = batch_targets.to(device)\n",
        "\n",
        "            # Generate paraphrases\n",
        "            generated_paraphrases = []\n",
        "            for input_seq in batch_inputs:\n",
        "                input_seq = input_seq.unsqueeze(0)\n",
        "                output = model(input_seq)\n",
        "                predicted_seq = torch.argmax(output, dim=-1).squeeze()\n",
        "                paraphrase = tokenizer.decode(predicted_seq, skip_special_tokens=True)\n",
        "                generated_paraphrases.append(paraphrase)\n",
        "\n",
        "            # Calculate BLEU score\n",
        "            for reference, candidate in zip(batch_targets, generated_paraphrases):\n",
        "                reference_text = tokenizer.decode(reference, skip_special_tokens=True)\n",
        "                candidate_text = candidate\n",
        "                bleu_score = sentence_bleu([reference_text.split()], candidate_text.split())\n",
        "                bleu_scores.append(bleu_score)\n",
        "\n",
        "            # Calculate semantic similarity\n",
        "            reference_embeddings = sentence_transformer.encode([tokenizer.decode(ref, skip_special_tokens=True) for ref in batch_targets])\n",
        "            candidate_embeddings = sentence_transformer.encode(generated_paraphrases)\n",
        "            for reference_embedding, candidate_embedding in zip(reference_embeddings, candidate_embeddings):\n",
        "                similarity_score = torch.cosine_similarity(torch.tensor(reference_embedding), torch.tensor(candidate_embedding), dim=0)\n",
        "                semantic_similarities.append(similarity_score.item())\n",
        "\n",
        "            # Calculate perplexity\n",
        "            for paraphrase in generated_paraphrases:\n",
        "                input_ids = gpt2_tokenizer.encode(paraphrase, return_tensors='pt')\n",
        "                with torch.no_grad():\n",
        "                    outputs = gpt2_model(input_ids, labels=input_ids)\n",
        "                    loss = outputs.loss\n",
        "                    perplexity = torch.exp(loss).item()\n",
        "                    perplexities.append(perplexity)\n",
        "\n",
        "    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "    avg_semantic_similarity = sum(semantic_similarities) / len(semantic_similarities)\n",
        "    avg_perplexity = sum(perplexities) / len(perplexities)\n",
        "\n",
        "    return avg_bleu_score, avg_semantic_similarity, avg_perplexity\n",
        "\n",
        "# Example usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "avg_bleu_score, avg_semantic_similarity, avg_perplexity = calculate_metrics(model, tokenizer, test_loader, device)\n",
        "\n",
        "print(f\"Average BLEU Score: {avg_bleu_score:.4f}\")\n",
        "print(f\"Average Semantic Similarity: {avg_semantic_similarity:.4f}\")\n",
        "print(f\"Average Perplexity: {avg_perplexity:.4f}\")"
      ],
      "metadata": {
        "id": "MEkMlIAuSSUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Human Evaluation through Implementation\n",
        "def paraphrase_sentence(model, tokenizer, input_sentence, max_length=50):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        input_tokens = tokenizer.texts_to_sequences([input_sentence])\n",
        "        input_tokens = torch.tensor(input_tokens, dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "        output_sentence = input_tokens.clone()\n",
        "\n",
        "\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            predictions = model(output_sentence)\n",
        "            _, predicted_index = torch.max(predictions, dim=-1)\n",
        "\n",
        "\n",
        "            output_sentence = torch.cat([output_sentence, predicted_index.unsqueeze(1)], dim=-1)\n",
        "\n",
        "\n",
        "            if predicted_index.item() == tokenizer.word_index.get('<end>', None):\n",
        "                break\n",
        "\n",
        "\n",
        "        paraphrase = tokenizer.sequences_to_texts(output_sentence.cpu().numpy())[0]\n",
        "\n",
        "        return paraphrase\n",
        "\n",
        "sentence = \"The bank shall, however, continue to provide services to its existing customers, including its credit card customers\"\n",
        "paraphrase = paraphrase_sentence(sentence)\n",
        "print(\"Original Sentence:\")\n",
        "print(sentence)\n",
        "print(\"Paraphrased Sentence:\")\n",
        "print(paraphrase)\n"
      ],
      "metadata": {
        "id": "WsvY_zauU7Y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pko6idINuW2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "3-iU9A_xeDMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BiaxGb5v9QuA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}